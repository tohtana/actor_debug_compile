[2025-06-20 22:33:35,962] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 22:33:36,081] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 22:33:37,216] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Starting at: 2025-06-20 22:33:37.773331

1. Initializing actor ...
Initializing actor model for debugging...
Note: Running in standalone mode (no distributed training)
Using model: meta-llama/Llama-3.1-8B-Instruct
Flash Attention: True
BF16: True
PyTorch Compile: False
Packing Samples: False
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 112.75it/s]
Actor model initialized successfully!
Actor model parameters: 8030261248
Actor model moved to GPU: 0
Tokenizer configured. Vocab size: 128256
Actor initialized successfully on device: 0

2. Loading input dump ...

3. Replaying an input ...

=== Replaying Input ===
Source: actor_inputs_rank0_20250619_052658_128923.pkl
Timestamp: 20250619_052658_128923
Rank: 0
Batch size: 2
Sequence length: 1259
[INFO 06-20 22:33:42.757 actor.py:213] [localhost][804090][rNA] Actor forward start. sequences=torch.Size([2, 1259])  padded_sequences=torch.Size([2, 1259]) stride (1259, 1) storage_offset 0 foward_attention_mask=torch.Size([2, 1259]) position_ids=torch.Size([2, 1259]) padded_position_ids=torch.Size([2, 1259]) stride (1259, 1) storage_offset 0
[INFO 06-20 22:33:42.757 actor.py:213] [localhost][804090][rNA] Actor forward start. sequences=torch.Size([2, 1259])  padded_sequences=torch.Size([2, 1259]) stride (1259, 1) storage_offset 0 foward_attention_mask=torch.Size([2, 1259]) position_ids=torch.Size([2, 1259]) padded_position_ids=torch.Size([2, 1259]) stride (1259, 1) storage_offset 0
[INFO 06-20 22:33:43.267 actor.py:216] [localhost][804090][rNA] Actor forward end
[INFO 06-20 22:33:43.267 actor.py:216] [localhost][804090][rNA] Actor forward end
Replay successful!
DEBUG:filelock:Attempting to acquire lock 139026233433040 on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Lock 139026233433040 acquired on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Attempting to release lock 139026233433040 on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Lock 139026233433040 released on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Attempting to acquire lock 139026232866064 on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Lock 139026232866064 acquired on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Attempting to release lock 139026232866064 on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Lock 139026232866064 released on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
