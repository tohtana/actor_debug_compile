[2025-06-20 22:36:00,763] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 22:36:00,890] [INFO] [real_accelerator.py:254:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-06-20 22:36:02,065] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Starting at: 2025-06-20 22:36:02.625123

1. Initializing actor ...
Initializing actor model for debugging...
Note: Running in standalone mode (no distributed training)
Using model: meta-llama/Llama-3.1-8B-Instruct
Flash Attention: True
BF16: True
PyTorch Compile: True
Packing Samples: True
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 110.48it/s]
Actor model initialized successfully!
Actor model parameters: 8030261248
Actor model moved to GPU: 0
Tokenizer configured. Vocab size: 128256
Enabling PyTorch compilation (without DeepSpeed DeepCompile)...
PyTorch compilation enabled successfully!
Note: DeepSpeed config has deepcompile: False, but engine.compile() was called
Actor initialized successfully on device: 0

2. Loading input dump ...

3. Replaying an input ...

=== Replaying Input ===
Source: actor_inputs_rank0_20250619_052658_128923.pkl
Timestamp: 20250619_052658_128923
Rank: 0
Batch size: 2
Sequence length: 1259
[INFO 06-20 22:36:09.365 actor.py:213] [localhost][808041][rNA] Actor forward start. sequences=torch.Size([1, 2328])  padded_sequences=torch.Size([1, 2328]) stride (1, 1) storage_offset 0 foward_attention_mask=None position_ids=torch.Size([1, 2328]) padded_position_ids=torch.Size([1, 2328]) stride (1, 1) storage_offset 0
[INFO 06-20 22:36:09.365 actor.py:213] [localhost][808041][rNA] Actor forward start. sequences=torch.Size([1, 2328])  padded_sequences=torch.Size([1, 2328]) stride (1, 1) storage_offset 0 foward_attention_mask=None position_ids=torch.Size([1, 2328]) padded_position_ids=torch.Size([1, 2328]) stride (1, 1) storage_offset 0

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_padded_sequences_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_padded_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_padded_sequences_ = L_padded_sequences_
        l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_
        l_padded_position_ids_ = L_padded_position_ids_
        l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_padded_sequences_, l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_padded_sequences_ = l_self_modules_model_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_padded_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_padded_position_ids_, dim = -1);  l_padded_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_0_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_0_ = L_args_0_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_0_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_0_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_1_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_1_ = L_args_1_
        l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_1_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_1_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_1_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_1_ = L_args_1_
        l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_1_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_1_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_1_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_1_ = L_args_1_
        l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_1_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_1_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_1_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_1_ = L_args_1_
        l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_1_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_1_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_1_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_1_ = L_args_1_
        l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_1_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_1_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_1_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_1_ = L_args_1_
        l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_1_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_1_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_args_1_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_args_1_ = L_args_1_
        l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = L_args_0_modules_model_modules_embed_tokens_parameters_weight_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_args_1_, l_args_0_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_args_1_ = l_args_0_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_args_0_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_kwargs_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_args_0_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_args_0_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_model_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_model_modules_embed_tokens_parameters_weight_ = L_self_modules_model_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_model_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_model_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_model_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_model_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_model_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_model_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_model_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_embed_tokens_parameters_weight_ = L_self_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_embed_tokens_parameters_weight_ = L_self_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_embed_tokens_parameters_weight_ = L_self_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_input_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_embed_tokens_parameters_weight_: "bf16[128256, 4096][4096, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0", L_self_modules_rotary_emb_buffers_inv_freq_: "f32[64][1]cuda:0", L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0"):
        l_input_ids_ = L_input_ids_
        l_self_modules_embed_tokens_parameters_weight_ = L_self_modules_embed_tokens_parameters_weight_
        l_position_ids_ = L_position_ids_
        l_self_modules_rotary_emb_buffers_inv_freq_ = L_self_modules_rotary_emb_buffers_inv_freq_
        l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = L_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:527 in forward, code: inputs_embeds = self.embed_tokens(input_ids)
        inputs_embeds: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch.nn.functional.embedding(l_input_ids_, l_self_modules_embed_tokens_parameters_weight_, None, None, 2.0, False, False);  l_input_ids_ = l_self_modules_embed_tokens_parameters_weight_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:534 in forward, code: cache_position = torch.arange(
        cache_position: "i64[2328][1]cuda:0" = torch.arange(0, 2328, device = device(type='cuda', index=0));  cache_position = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:113 in forward, code: inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)
        getitem: "f32[1, 64, 1][64, 1, 1]cuda:0" = l_self_modules_rotary_emb_buffers_inv_freq_[(None, slice(None, None, None), None)];  l_self_modules_rotary_emb_buffers_inv_freq_ = None
        float_1: "f32[1, 64, 1][64, 1, 1]cuda:0" = getitem.float();  getitem = None
        expand: "f32[1, 64, 1][64, 1, 1]cuda:0" = float_1.expand(1, -1, 1);  float_1 = None
        inv_freq_expanded: "f32[1, 64, 1][64, 1, 1]cuda:0" = expand.to(device(type='cuda', index=0));  expand = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:114 in forward, code: position_ids_expanded = position_ids[:, None, :].float()
        getitem_1: "i64[1, 1, 2328][1, 2328, 1]cuda:0" = l_position_ids_[(slice(None, None, None), None, slice(None, None, None))]
        position_ids_expanded: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = getitem_1.float();  getitem_1 = None
        
        # No stacktrace found for following nodes
        _enter_autocast = torch.amp.autocast_mode._enter_autocast('cuda', None, False, None)
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:118 in forward, code: freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)
        float_3: "f32[1, 64, 1][64, 1, 1]cuda:0" = inv_freq_expanded.float();  inv_freq_expanded = None
        float_4: "f32[1, 1, 2328][1, 2328, 1]cuda:0" = position_ids_expanded.float();  position_ids_expanded = None
        matmul: "f32[1, 64, 2328][148992, 2328, 1]cuda:0" = float_3 @ float_4;  float_3 = float_4 = None
        freqs: "f32[1, 2328, 64][148992, 1, 2328]cuda:0" = matmul.transpose(1, 2);  matmul = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:119 in forward, code: emb = torch.cat((freqs, freqs), dim=-1)
        emb: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = torch.cat((freqs, freqs), dim = -1);  freqs = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:120 in forward, code: cos = emb.cos() * self.attention_scaling
        cos: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.cos()
        cos_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = cos * 1.0;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:121 in forward, code: sin = emb.sin() * self.attention_scaling
        sin: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = emb.sin();  emb = None
        sin_1: "f32[1, 2328, 128][297984, 128, 1]cuda:0" = sin * 1.0;  sin = None
        
        # No stacktrace found for following nodes
        _exit_autocast = torch.amp.autocast_mode._exit_autocast(_enter_autocast);  _enter_autocast = _exit_autocast = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:123 in forward, code: return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)
        cos_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = cos_1.to(dtype = torch.bfloat16);  cos_1 = None
        sin_2: "bf16[1, 2328, 128][297984, 128, 1]cuda:0" = sin_1.to(dtype = torch.bfloat16);  sin_1 = None
        
         # File: /opt/conda/lib/python3.11/site-packages/torch/_dynamo/polyfills/__init__.py:156 in instantiate_user_defined_class_object, code: obj.__init__(*args, **kwargs)
        _log_api_usage_once = torch._C._log_api_usage_once('python.nn_module');  _log_api_usage_once = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = inputs_embeds.to(torch.float32);  inputs_embeds = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_4: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ * to_4;  l_self_modules_layers_modules_0_modules_input_layernorm_parameters_weight_ = to_4 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_layers_modules_0_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_layers_modules_0_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = cos_2.unsqueeze(1);  cos_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin_3: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = sin_2.unsqueeze(1);  sin_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_4: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos_3
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_5: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat_1 * sin_3;  cat_1 = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_6: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos_3;  cos_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_2: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_7: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_2 * sin_3;  cat_2 = sin_3 = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_6 + mul_7;  mul_6 = mul_7 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_hidden_states_: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0", L_self_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_position_embeddings_0_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_embeddings_1_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0"):
        l_hidden_states_ = L_hidden_states_
        l_self_modules_input_layernorm_parameters_weight_ = L_self_modules_input_layernorm_parameters_weight_
        l_self_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_self_attn_modules_v_proj_parameters_weight_
        l_position_embeddings_0_ = L_position_embeddings_0_
        l_position_embeddings_1_ = L_position_embeddings_1_
        l_position_ids_ = L_position_ids_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_hidden_states_.to(torch.float32);  l_hidden_states_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_1: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_input_layernorm_parameters_weight_ = to_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_0_.unsqueeze(1);  l_position_embeddings_0_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_1_.unsqueeze(1);  l_position_embeddings_1_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat * sin;  cat = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_1: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_1 * sin;  cat_1 = sin = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_hidden_states_: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0", L_self_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_position_embeddings_0_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_embeddings_1_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0"):
        l_hidden_states_ = L_hidden_states_
        l_self_modules_input_layernorm_parameters_weight_ = L_self_modules_input_layernorm_parameters_weight_
        l_self_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_self_attn_modules_v_proj_parameters_weight_
        l_position_embeddings_0_ = L_position_embeddings_0_
        l_position_embeddings_1_ = L_position_embeddings_1_
        l_position_ids_ = L_position_ids_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_hidden_states_.to(torch.float32);  l_hidden_states_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_1: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_input_layernorm_parameters_weight_ = to_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_0_.unsqueeze(1);  l_position_embeddings_0_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_1_.unsqueeze(1);  l_position_embeddings_1_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat * sin;  cat = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_1: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_1 * sin;  cat_1 = sin = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_hidden_states_: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0", L_self_modules_input_layernorm_parameters_weight_: "bf16[4096][1]cuda:0", L_self_modules_self_attn_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_self_attn_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_self_attn_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_position_embeddings_0_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_embeddings_1_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_ids_: "i64[1, 2328][1, 1]cuda:0"):
        l_hidden_states_ = L_hidden_states_
        l_self_modules_input_layernorm_parameters_weight_ = L_self_modules_input_layernorm_parameters_weight_
        l_self_modules_self_attn_modules_q_proj_parameters_weight_ = L_self_modules_self_attn_modules_q_proj_parameters_weight_
        l_self_modules_self_attn_modules_k_proj_parameters_weight_ = L_self_modules_self_attn_modules_k_proj_parameters_weight_
        l_self_modules_self_attn_modules_v_proj_parameters_weight_ = L_self_modules_self_attn_modules_v_proj_parameters_weight_
        l_position_embeddings_0_ = L_position_embeddings_0_
        l_position_embeddings_1_ = L_position_embeddings_1_
        l_position_ids_ = L_position_ids_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:80 in forward, code: hidden_states = hidden_states.to(torch.float32)
        hidden_states: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_hidden_states_.to(torch.float32);  l_hidden_states_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:81 in forward, code: variance = hidden_states.pow(2).mean(-1, keepdim=True)
        pow_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states.pow(2)
        variance: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = pow_1.mean(-1, keepdim = True);  pow_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:82 in forward, code: hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        add: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = variance + 1e-05;  variance = None
        rsqrt: "f32[1, 2328, 1][2328, 1, 1]cuda:0" = torch.rsqrt(add);  add = None
        hidden_states_1: "f32[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states * rsqrt;  hidden_states = rsqrt = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:83 in forward, code: return self.weight * hidden_states.to(input_dtype)
        to_1: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = hidden_states_1.to(torch.bfloat16);  hidden_states_1 = None
        hidden_states_2: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = l_self_modules_input_layernorm_parameters_weight_ * to_1;  l_self_modules_input_layernorm_parameters_weight_ = to_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_q_proj_parameters_weight_, None);  l_self_modules_self_attn_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_k_proj_parameters_weight_, None);  l_self_modules_self_attn_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(hidden_states_2, l_self_modules_self_attn_modules_v_proj_parameters_weight_, None);  hidden_states_2 = l_self_modules_self_attn_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_0_.unsqueeze(1);  l_position_embeddings_0_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_1_.unsqueeze(1);  l_position_embeddings_1_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_2: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_3: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat * sin;  cat = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_4: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_1: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_5: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_1 * sin;  cat_1 = sin = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_4 + mul_5;  mul_4 = mul_5 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_position_ids_, dim = -1);  l_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_hidden_states_: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0", L_self_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_position_embeddings_0_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_embeddings_1_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0"):
        l_hidden_states_ = L_hidden_states_
        l_self_modules_q_proj_parameters_weight_ = L_self_modules_q_proj_parameters_weight_
        l_self_modules_k_proj_parameters_weight_ = L_self_modules_k_proj_parameters_weight_
        l_self_modules_v_proj_parameters_weight_ = L_self_modules_v_proj_parameters_weight_
        l_position_embeddings_0_ = L_position_embeddings_0_
        l_position_embeddings_1_ = L_position_embeddings_1_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(l_hidden_states_, l_self_modules_q_proj_parameters_weight_, None);  l_self_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(l_hidden_states_, l_self_modules_k_proj_parameters_weight_, None);  l_self_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(l_hidden_states_, l_self_modules_v_proj_parameters_weight_, None);  l_hidden_states_ = l_self_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_0_.unsqueeze(1);  l_position_embeddings_0_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_1_.unsqueeze(1);  l_position_embeddings_1_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat * sin;  cat = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul + mul_1;  mul = mul_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_1: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_1 * sin;  cat_1 = sin = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_hidden_states_: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0", L_self_modules_q_proj_parameters_weight_: "bf16[4096, 4096][4096, 1]cuda:0", L_self_modules_k_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_self_modules_v_proj_parameters_weight_: "bf16[1024, 4096][4096, 1]cuda:0", L_position_embeddings_0_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_position_embeddings_1_: "bf16[1, 2328, 128][297984, 128, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0"):
        l_hidden_states_ = L_hidden_states_
        l_self_modules_q_proj_parameters_weight_ = L_self_modules_q_proj_parameters_weight_
        l_self_modules_k_proj_parameters_weight_ = L_self_modules_k_proj_parameters_weight_
        l_self_modules_v_proj_parameters_weight_ = L_self_modules_v_proj_parameters_weight_
        l_position_embeddings_0_ = L_position_embeddings_0_
        l_position_embeddings_1_ = L_position_embeddings_1_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:252 in forward, code: query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear: "bf16[1, 2328, 4096][9535488, 4096, 1]cuda:0" = torch._C._nn.linear(l_hidden_states_, l_self_modules_q_proj_parameters_weight_, None);  l_self_modules_q_proj_parameters_weight_ = None
        view: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = linear.view((1, 2328, -1, 128));  linear = None
        query_states: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = view.transpose(1, 2);  view = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:253 in forward, code: key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_1: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(l_hidden_states_, l_self_modules_k_proj_parameters_weight_, None);  l_self_modules_k_proj_parameters_weight_ = None
        view_1: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_1.view((1, 2328, -1, 128));  linear_1 = None
        key_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_1.transpose(1, 2);  view_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:254 in forward, code: value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)
        linear_2: "bf16[1, 2328, 1024][2383872, 1024, 1]cuda:0" = torch._C._nn.linear(l_hidden_states_, l_self_modules_v_proj_parameters_weight_, None);  l_hidden_states_ = l_self_modules_v_proj_parameters_weight_ = None
        view_2: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = linear_2.view((1, 2328, -1, 128));  linear_2 = None
        value_states: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = view_2.transpose(1, 2);  view_2 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:153 in apply_rotary_pos_emb, code: cos = cos.unsqueeze(unsqueeze_dim)
        cos: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_0_.unsqueeze(1);  l_position_embeddings_0_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:154 in apply_rotary_pos_emb, code: sin = sin.unsqueeze(unsqueeze_dim)
        sin: "bf16[1, 1, 2328, 128][297984, 297984, 128, 1]cuda:0" = l_position_embeddings_1_.unsqueeze(1);  l_position_embeddings_1_ = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = query_states * cos
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2: "bf16[1, 32, 2328, 64][9535488, 128, 4096, 1]cuda:0" = query_states[(Ellipsis, slice(64, None, None))];  query_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg: "bf16[1, 32, 2328, 64][4767744, 64, 2048, 1]cuda:0" = -x2;  x2 = None
        cat: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = torch.cat((neg, x1), dim = -1);  neg = x1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:155 in apply_rotary_pos_emb, code: q_embed = (q * cos) + (rotate_half(q) * sin)
        mul_1: "bf16[1, 32, 2328, 128][9535488, 297984, 128, 1]cuda:0" = cat * sin;  cat = None
        q_embed: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0" = mul + mul_1;  mul = mul_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_2: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = key_states * cos;  cos = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:128 in rotate_half, code: x1 = x[..., : x.shape[-1] // 2]
        x1_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(None, 64, None))]
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:129 in rotate_half, code: x2 = x[..., x.shape[-1] // 2 :]
        x2_1: "bf16[1, 8, 2328, 64][2383872, 128, 1024, 1]cuda:0" = key_states[(Ellipsis, slice(64, None, None))];  key_states = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:130 in rotate_half, code: return torch.cat((-x2, x1), dim=-1)
        neg_1: "bf16[1, 8, 2328, 64][1191936, 64, 512, 1]cuda:0" = -x2_1;  x2_1 = None
        cat_1: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = torch.cat((neg_1, x1_1), dim = -1);  neg_1 = x1_1 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:156 in apply_rotary_pos_emb, code: k_embed = (k * cos) + (rotate_half(k) * sin)
        mul_3: "bf16[1, 8, 2328, 128][2383872, 297984, 128, 1]cuda:0" = cat_1 * sin;  cat_1 = sin = None
        k_embed: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0" = mul_2 + mul_3;  mul_2 = mul_3 = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = q_embed.transpose(1, 2);  q_embed = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = k_embed.transpose(1, 2);  k_embed = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = value_states.transpose(1, 2);  value_states = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        

class GraphModule(torch.nn.Module):
    def forward(self, L_query_: "bf16[1, 32, 2328, 128][9535488, 128, 4096, 1]cuda:0", L_key_: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0", L_value_: "bf16[1, 8, 2328, 128][2383872, 128, 1024, 1]cuda:0", L_kwargs_position_ids_: "i64[1, 2328][1, 1]cuda:0"):
        l_query_ = L_query_
        l_key_ = L_key_
        l_value_ = L_value_
        l_kwargs_position_ids_ = L_kwargs_position_ids_
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:27 in flash_attention_forward, code: query = query.transpose(1, 2)
        query: "bf16[1, 2328, 32, 128][9535488, 4096, 128, 1]cuda:0" = l_query_.transpose(1, 2);  l_query_ = query = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:28 in flash_attention_forward, code: key = key.transpose(1, 2)
        key: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = l_key_.transpose(1, 2);  l_key_ = key = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py:29 in flash_attention_forward, code: value = value.transpose(1, 2)
        value: "bf16[1, 2328, 8, 128][2383872, 1024, 128, 1]cuda:0" = l_value_.transpose(1, 2);  l_value_ = value = None
        
         # File: /home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py:378 in _flash_attention_forward, code: max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
        diff: "i64[1, 2327][2327, 1]cuda:0" = torch.diff(l_kwargs_position_ids_, dim = -1);  l_kwargs_position_ids_ = None
        ge: "b8[1, 2327][2327, 1]cuda:0" = diff >= 0;  diff = None
        all_1: "b8[][]cuda:0" = ge.all();  ge = all_1 = None
        
W0620 22:36:11.551000 808041 site-packages/torch/fx/experimental/symbolic_shapes.py:6679] [26/0] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0620 22:36:11.551000 808041 site-packages/torch/fx/experimental/recording.py:299] [26/0] failed while running evaluate_expr(*(u0, None, False, False), **{})
W0620 22:36:11.552000 808041 site-packages/torch/fx/experimental/symbolic_shapes.py:6679] [26/0] failed during evaluate_expr(u0, hint=None, size_oblivious=False, forcing_spec=False
E0620 22:36:11.552000 808041 site-packages/torch/fx/experimental/recording.py:299] [26/0] failed while running evaluate_expr(*(u0, None, False, False), **{})
Traceback (most recent call last):
  File "/home/mtanaka/work/dc/actor_debug_simple/replay_inputs.py", line 67, in replay_single_input
    result = actor(
             ^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1749, in _wrapped_call_impl
    return self._compiled_call_impl(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py", line 655, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/work/dc/actor_debug_simple/../OpenRLHF/openrlhf/models/actor.py", line 189, in forward
    sequences, position_ids, rolled_sequences, ring_attn_pad_len, indices = unpad_and_slice_tensor(
  File "/home/mtanaka/work/dc/actor_debug_simple/../OpenRLHF/openrlhf/models/actor.py", line 210, in torch_dynamo_resume_in_forward_at_189
    logger = logging.getLogger("openrlhf")
  File "/home/mtanaka/work/dc/actor_debug_simple/../OpenRLHF/openrlhf/models/actor.py", line 211, in torch_dynamo_resume_in_forward_at_210
    logging.basicConfig(level=logging.DEBUG, force=True)
  File "/home/mtanaka/work/dc/actor_debug_simple/../OpenRLHF/openrlhf/models/actor.py", line 213, in torch_dynamo_resume_in_forward_at_211
    logger.info(f"Actor forward start. sequences={sequences.shape}  padded_sequences={padded_sequences.shape} stride {padded_sequences.stride()} storage_offset {padded_sequences.storage_offset()} foward_attention_mask={foward_attention_mask.shape if foward_attention_mask is not None else None} position_ids={position_ids.shape} padded_position_ids={padded_position_ids.shape} stride {padded_position_ids.stride()} storage_offset {padded_position_ids.storage_offset()}")
  File "/home/mtanaka/work/dc/actor_debug_simple/../OpenRLHF/openrlhf/models/actor.py", line 213, in torch_dynamo_resume_in_forward_at_213
    logger.info(f"Actor forward start. sequences={sequences.shape}  padded_sequences={padded_sequences.shape} stride {padded_sequences.stride()} storage_offset {padded_sequences.storage_offset()} foward_attention_mask={foward_attention_mask.shape if foward_attention_mask is not None else None} position_ids={position_ids.shape} padded_position_ids={padded_position_ids.shape} stride {padded_position_ids.stride()} storage_offset {padded_position_ids.storage_offset()}")
  File "/home/mtanaka/work/dc/actor_debug_simple/../OpenRLHF/openrlhf/models/actor.py", line 213, in torch_dynamo_resume_in_forward_at_213
    logger.info(f"Actor forward start. sequences={sequences.shape}  padded_sequences={padded_sequences.shape} stride {padded_sequences.stride()} storage_offset {padded_sequences.storage_offset()} foward_attention_mask={foward_attention_mask.shape if foward_attention_mask is not None else None} position_ids={position_ids.shape} padded_position_ids={padded_position_ids.shape} stride {padded_position_ids.stride()} storage_offset {padded_position_ids.storage_offset()}")
  File "/home/mtanaka/work/dc/actor_debug_simple/../OpenRLHF/openrlhf/models/actor.py", line 214, in torch_dynamo_resume_in_forward_at_213
    output = self.model(padded_sequences, attention_mask=foward_attention_mask, position_ids=padded_position_ids)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 821, in forward
    outputs: BaseModelOutputWithPast = self.model(
                                       ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/utils/generic.py", line 965, in wrapper
    output = func(self, *args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 571, in forward
    layer_outputs = decoder_layer(
                    ^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 318, in forward
    hidden_states, self_attn_weights = self.self_attn(
                                       ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1751, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1762, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py", line 274, in forward
    attn_output, attn_weights = attention_interface(
                                ^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/integrations/flash_attention.py", line 49, in flash_attention_forward
    attn_output = _flash_attention_forward(
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 378, in _flash_attention_forward
    max_length_q is not None or (query_length != 1 and not (torch.diff(position_ids, dim=-1) >= 0).all())
  File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 384, in torch_dynamo_resume_in__flash_attention_forward_at_378
    prepare_fa2_from_position_ids(query_states, key_states, value_states, position_ids)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1432, in __call__
    return self._torchdynamo_orig_callable(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1213, in __call__
    result = self._inner_convert(
             ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 598, in __call__
    return _compile(
           ^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 1059, in _compile
    guarded_code = compile_inner(code, one_graph, hooks, transform)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_utils_internal.py", line 97, in wrapper_function
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 761, in compile_inner
    return _compile_inner(code, one_graph, hooks, transform)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 797, in _compile_inner
    out_code = transform_code_object(code, transform)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/bytecode_transformation.py", line 1422, in transform_code_object
    transformations(instructions, code_options)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 257, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/convert_frame.py", line 715, in transform
    tracer.run()
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 3500, in run
    super().run()
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1337, in run
    while self.step():
          ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1246, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 819, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2266, in CALL_FUNCTION_EX
    self.call_function(fn, argsvars.items, kwargsvars)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1170, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 404, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 185, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1187, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 3726, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 3905, in inline_call_
    self.run()
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1337, in run
    while self.step():
          ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1246, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 819, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2933, in CALL
    self._call(inst)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2927, in _call
    self.call_function(fn, args, kwargs)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1170, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py", line 903, in call_function
    return self.obj.call_method(tx, self.name, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py", line 640, in call_method
    return self.call_apply(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/misc.py", line 589, in call_apply
    return variables.UserFunctionVariable(fn, source=source).call_function(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 404, in call_function
    return super().call_function(tx, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/functions.py", line 185, in call_function
    return tx.inline_user_function_return(self, [*self.self_args(), *args], kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1187, in inline_user_function_return
    return InliningInstructionTranslator.inline_call(self, fn, args, kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 3726, in inline_call
    return tracer.inline_call_()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 3905, in inline_call_
    self.run()
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1337, in run
    while self.step():
          ^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1246, in step
    self.dispatch_table[inst.opcode](self, inst)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 819, in wrapper
    return inner_fn(self, inst)
           ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2933, in CALL
    self._call(inst)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 2927, in _call
    self.call_function(fn, args, kwargs)
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/symbolic_convert.py", line 1170, in call_function
    self.push(fn.call_function(self, args, kwargs))  # type: ignore[arg-type]
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/torch.py", line 1181, in call_function
    tensor_variable = wrap_fx_proxy(
                      ^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 2302, in wrap_fx_proxy
    return wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 2368, in wrap_fx_proxy_cls
    return _wrap_fx_proxy(
           ^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/variables/builder.py", line 2464, in _wrap_fx_proxy
    example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=True)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 3229, in get_fake_value
    raise TorchRuntimeError(str(e)).with_traceback(e.__traceback__) from None
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 3127, in get_fake_value
    ret_val = wrap_fake_exception(
              ^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 2641, in wrap_fake_exception
    return fn()
           ^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 3128, in <lambda>
    lambda: run_node(tx.output, node, args, kwargs, nnmodule)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 3325, in run_node
    raise RuntimeError(make_error_message(e)).with_traceback(
  File "/opt/conda/lib/python3.11/site-packages/torch/_dynamo/utils.py", line 3284, in run_node
    return node.target(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/conda/lib/python3.11/site-packages/torch/_ops.py", line 1158, in __call__
    return self._op(*args, **(kwargs or {}))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch._dynamo.exc.TorchRuntimeError: Dynamo failed to run FX node with fake tensors: call_function flash_attn._flash_attn_varlen_forward(*(FakeTensor(..., device='cuda:0', size=(2328, 32, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(2328, 8, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(2328, 8, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(3,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(3,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), 0.0, 0.08838834764831845), **{'causal': True, 'window_size_left': -1, 'window_size_right': -1, 'softcap': 0.0, 'alibi_slopes': None, 'return_softmax': False, 'block_table': None}): got RuntimeError("flash_attn::_flash_attn_varlen_forward() Expected a value of type 'int' for argument 'max_seqlen_q' but instead found type 'FakeTensor'.\nPosition: 5\nValue: FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64)\nDeclaration: flash_attn::_flash_attn_varlen_forward(Tensor q, Tensor k, Tensor v, Tensor cu_seqlens_q, Tensor cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left=-1, SymInt window_size_right=-1, float softcap=0., Tensor? alibi_slopes=None, bool return_softmax=False, Tensor? block_table=None, Tensor? leftpad_k=None, Tensor? seqused_k=None, bool zero_tensors=False) -> (Tensor, Tensor, Tensor, Tensor)\nCast error details: Unable to cast Python instance of type <class 'torch._subclasses.fake_tensor.FakeTensor'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)")

from user code:
   File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 395, in torch_dynamo_resume_in__flash_attention_forward_at_384
    attn_output = flash_attn_varlen_func(
  File "/home/mtanaka/.local/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 1448, in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
  File "/home/mtanaka/.local/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 930, in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(

Error during forward pass: Dynamo failed to run FX node with fake tensors: call_function flash_attn._flash_attn_varlen_forward(*(FakeTensor(..., device='cuda:0', size=(2328, 32, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(2328, 8, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(2328, 8, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(3,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(3,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), 0.0, 0.08838834764831845), **{'causal': True, 'window_size_left': -1, 'window_size_right': -1, 'softcap': 0.0, 'alibi_slopes': None, 'return_softmax': False, 'block_table': None}): got RuntimeError("flash_attn::_flash_attn_varlen_forward() Expected a value of type 'int' for argument 'max_seqlen_q' but instead found type 'FakeTensor'.\nPosition: 5\nValue: FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64)\nDeclaration: flash_attn::_flash_attn_varlen_forward(Tensor q, Tensor k, Tensor v, Tensor cu_seqlens_q, Tensor cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left=-1, SymInt window_size_right=-1, float softcap=0., Tensor? alibi_slopes=None, bool return_softmax=False, Tensor? block_table=None, Tensor? leftpad_k=None, Tensor? seqused_k=None, bool zero_tensors=False) -> (Tensor, Tensor, Tensor, Tensor)\nCast error details: Unable to cast Python instance of type <class 'torch._subclasses.fake_tensor.FakeTensor'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)")

from user code:
   File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 395, in torch_dynamo_resume_in__flash_attention_forward_at_384
    attn_output = flash_attn_varlen_func(
  File "/home/mtanaka/.local/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 1448, in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
  File "/home/mtanaka/.local/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 930, in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(

Replay failed with error: Dynamo failed to run FX node with fake tensors: call_function flash_attn._flash_attn_varlen_forward(*(FakeTensor(..., device='cuda:0', size=(2328, 32, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(2328, 8, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(2328, 8, 128), dtype=torch.bfloat16), FakeTensor(..., device='cuda:0', size=(3,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(3,), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64), 0.0, 0.08838834764831845), **{'causal': True, 'window_size_left': -1, 'window_size_right': -1, 'softcap': 0.0, 'alibi_slopes': None, 'return_softmax': False, 'block_table': None}): got RuntimeError("flash_attn::_flash_attn_varlen_forward() Expected a value of type 'int' for argument 'max_seqlen_q' but instead found type 'FakeTensor'.\nPosition: 5\nValue: FakeTensor(..., device='cuda:0', size=(), dtype=torch.int64)\nDeclaration: flash_attn::_flash_attn_varlen_forward(Tensor q, Tensor k, Tensor v, Tensor cu_seqlens_q, Tensor cu_seqlens_k, SymInt max_seqlen_q, SymInt max_seqlen_k, float dropout_p, float softmax_scale, bool causal, SymInt window_size_left=-1, SymInt window_size_right=-1, float softcap=0., Tensor? alibi_slopes=None, bool return_softmax=False, Tensor? block_table=None, Tensor? leftpad_k=None, Tensor? seqused_k=None, bool zero_tensors=False) -> (Tensor, Tensor, Tensor, Tensor)\nCast error details: Unable to cast Python instance of type <class 'torch._subclasses.fake_tensor.FakeTensor'> to C++ type '?' (#define PYBIND11_DETAILED_ERROR_MESSAGES or compile in debug mode for details)")

from user code:
   File "/home/mtanaka/.local/lib/python3.11/site-packages/transformers/modeling_flash_attention_utils.py", line 395, in torch_dynamo_resume_in__flash_attention_forward_at_384
    attn_output = flash_attn_varlen_func(
  File "/home/mtanaka/.local/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 1448, in flash_attn_varlen_func
    return FlashAttnVarlenFunc.apply(
  File "/home/mtanaka/.local/lib/python3.11/site-packages/flash_attn/flash_attn_interface.py", line 930, in forward
    out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_varlen_forward(

DEBUG:filelock:Attempting to acquire lock 140298488200784 on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Lock 140298488200784 acquired on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Attempting to release lock 140298488200784 on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Lock 140298488200784 released on /home/mtanaka/.triton/autotune/Fp16Matmul_2d_kernel.pickle.lock
DEBUG:filelock:Attempting to acquire lock 140298127537168 on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Lock 140298127537168 acquired on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Attempting to release lock 140298127537168 on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
DEBUG:filelock:Lock 140298127537168 released on /home/mtanaka/.triton/autotune/Fp16Matmul_4d_kernel.pickle.lock
